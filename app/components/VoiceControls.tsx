'use client'

import { useState, useRef, useEffect } from 'react'
import VoiceOrb from './VoiceOrb'
import { AVAILABLE_TOOLS, TOOL_HANDLERS } from '../config/tools'
import { buildFinalSystemPrompt } from '../config/protected-prompt'
import { 
  DEFAULT_VOICE,
  REALTIME_VOICES,
  WEBRTC_CONFIG,
  MICROPHONE_CONFIG,
  ANALYSER_CONFIG,
  OUTPUT_ANALYSER_CONFIG,
  REALTIME_API_URLs,
  type RealtimeVoiceId 
} from '../config/realtime-config'

interface VoiceControlsProps {
  systemPrompt: string
  onVoiceStateChange: (state: 'disconnected' | 'connecting' | 'talking') => void
  onVolumeChange: (volume: number, type: 'input' | 'output') => void
  voiceState: 'disconnected' | 'connecting' | 'talking'
  inputVolume: number
  outputVolume: number
}

export default function VoiceControls({ systemPrompt, onVoiceStateChange, onVolumeChange, voiceState, inputVolume, outputVolume }: VoiceControlsProps) {
  const [isConnected, setIsConnected] = useState(false)
  const [isConnecting, setIsConnecting] = useState(false)
  const [isMuted, setIsMuted] = useState(false)
  const [selectedVoice, setSelectedVoice] = useState<RealtimeVoiceId>(DEFAULT_VOICE)
  
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const localStreamRef = useRef<MediaStream | null>(null)
  const remoteStreamRef = useRef<MediaStream | null>(null)
  const audioElementRef = useRef<HTMLAudioElement>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const inputAnalyserRef = useRef<AnalyserNode | null>(null)
  const outputAnalyserRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const volumeHistoryRef = useRef<{ input: number[], output: number[] }>({ input: [], output: [] })
  const lastVolumeUpdateRef = useRef<number>(0)

  // üîß –ù–û–í–´–ï REF'–´ –î–õ–Ø AUDIO RECOVERY SYSTEM
  const healthCheckIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const lastOutputDataRef = useRef<number>(0)
  const audioRecoveryAttemptsRef = useRef<number>(0)
  const maxRecoveryAttemptsRef = useRef<number>(3)

  // üéôÔ∏è –î–û–°–¢–£–ü–ù–´–ï –ì–û–õ–û–°–ê OpenAI Realtime API (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
  const availableVoices = REALTIME_VOICES

  // üéØ 1. AUDIOCTX RESILIENCE - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è suspending
  const ensureAudioContextActive = async (): Promise<boolean> => {
    if (!audioContextRef.current) {
      console.log('üîß AudioContext –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º...')
      return false
    }
    
    if (audioContextRef.current.state === 'suspended') {
      try {
        console.log('üîß AudioContext suspended, –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º...')
        await audioContextRef.current.resume()
        console.log('‚úÖ AudioContext —É—Å–ø–µ—à–Ω–æ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω')
        return true
      } catch (error) {
        console.error('‚ùå –û—à–∏–±–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è AudioContext:', error)
        return false
      }
    }
    
    if (audioContextRef.current.state === 'closed') {
      console.log('üîß AudioContext –∑–∞–∫—Ä—ã—Ç, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ')
      return false
    }
    
    return audioContextRef.current.state === 'running'
  }

  // üéØ 2. HTML AUDIO ELEMENT RECOVERY - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
  const setupAudioElementRecovery = (audioElement: HTMLAudioElement) => {
    console.log('üîß –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º audio element recovery')
    
    // –°–æ–±—ã—Ç–∏—è, —Ç—Ä–µ–±—É—é—â–∏–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
    const recoveryEvents = ['waiting', 'stalled', 'suspend', 'error', 'emptied', 'pause']
    
    recoveryEvents.forEach(eventType => {
      audioElement.addEventListener(eventType, handleAudioElementRecovery)
    })

    // –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –¥–ª—è —Å–±—Ä–æ—Å–∞ —Å—á–µ—Ç—á–∏–∫–∞ –ø–æ–ø—ã—Ç–æ–∫
    const successEvents = ['playing', 'canplaythrough', 'loadeddata']
    successEvents.forEach(eventType => {
      audioElement.addEventListener(eventType, () => {
        audioRecoveryAttemptsRef.current = 0
      })
    })
  }

  const handleAudioElementRecovery = async (event: Event) => {
    const eventType = event.type
    console.log(`üîß Audio element recovery triggered: ${eventType}`)
    
    if (audioRecoveryAttemptsRef.current >= maxRecoveryAttemptsRef.current) {
      console.log('‚ö†Ô∏è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ')
      return
    }
    
    audioRecoveryAttemptsRef.current++
    
    if (!audioElementRef.current || !remoteStreamRef.current) {
      console.log('‚ùå Audio element –∏–ª–∏ remote stream –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç')
      return
    }

    try {
      // –ü—ã—Ç–∞–µ–º—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–æ–±—ã—Ç–∏—è
      switch (eventType) {
        case 'waiting':
        case 'stalled':
          console.log('üîß –ü–æ–ø—ã—Ç–∫–∞ reload audio element...')
          audioElementRef.current.load()
          break
          
        case 'error':
        case 'emptied':
          console.log('üîß –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º srcObject...')
          audioElementRef.current.srcObject = null
          await new Promise(resolve => setTimeout(resolve, 100))
          audioElementRef.current.srcObject = remoteStreamRef.current
          break
          
        case 'suspend':
          console.log('üîß –ü–æ–ø—ã—Ç–∫–∞ resume playback...')
          if (audioElementRef.current.paused) {
            await audioElementRef.current.play().catch(console.error)
          }
          break
          
        case 'pause':
          // –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –±—ã–ª –Ω–∞–º–µ—Ä–µ–Ω–Ω—ã–π pause
          if (isConnected && !isMuted) {
            console.log('üîß –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞, –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º...')
            await audioElementRef.current.play().catch(console.error)
          }
          break
      }
      
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º AudioContext –ø–æ—Å–ª–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
      await ensureAudioContextActive()
      
    } catch (error) {
      console.error(`‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è audio element (${eventType}):`, error)
    }
  }

  // üéØ 3. MEDIASTREAM HEALTH CHECK - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–∞
  const performStreamHealthCheck = (): boolean => {
    if (!remoteStreamRef.current) {
      console.log('‚ö†Ô∏è Health check: remote stream –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')
      return false
    }
    
    const audioTracks = remoteStreamRef.current.getAudioTracks()
    if (audioTracks.length === 0) {
      console.log('‚ö†Ô∏è Health check: –Ω–µ—Ç audio tracks')
      return false
    }
    
    const activeTrack = audioTracks[0]
    if (activeTrack.readyState !== 'live') {
      console.log(`‚ö†Ô∏è Health check: track –Ω–µ live (${activeTrack.readyState})`)
      return false
    }
    
    if (activeTrack.muted) {
      console.log('‚ö†Ô∏è Health check: track muted')
      return false
    }
    
    return true
  }

  // üéØ 4. ANALYSER DATA VALIDATION - –ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—É–ø–∞—é—Ç
  const validateAnalyserData = (): boolean => {
    if (!outputAnalyserRef.current) return false
    
    const bufferLength = outputAnalyserRef.current.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    outputAnalyserRef.current.getByteFrequencyData(dataArray)
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –≤—Å–µ –Ω—É–ª–∏ (–µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)
    const hasData = dataArray.some(value => value > 0)
    const currentTime = performance.now()
    
    if (hasData) {
      lastOutputDataRef.current = currentTime
      return true
    }
    
    // –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –±–æ–ª—å—à–µ 5 —Å–µ–∫—É–Ω–¥, —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞
    const noDataDuration = currentTime - lastOutputDataRef.current
    if (noDataDuration > 5000) {
      console.log('‚ö†Ô∏è Analyser –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —É–∂–µ 5+ —Å–µ–∫—É–Ω–¥')
      return false
    }
    
    return true
  }

  // üéØ 5. COMPREHENSIVE HEALTH CHECK - –æ–±—â–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã
  const performComprehensiveHealthCheck = async () => {
    if (!isConnected) return
    
    let needsRecovery = false
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º AudioContext
    if (!(await ensureAudioContextActive())) {
      console.log('üîß Health check: AudioContext needs recovery')
      needsRecovery = true
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º MediaStream
    if (!performStreamHealthCheck()) {
      console.log('üîß Health check: MediaStream needs attention')
      needsRecovery = true
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º Analyser data flow
    if (!validateAnalyserData()) {
      console.log('üîß Health check: Analyser data flow interrupted')
      needsRecovery = true
      
      // –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å output analyser
      if (remoteStreamRef.current && audioContextRef.current) {
        try {
          const outputSource = audioContextRef.current.createMediaStreamSource(remoteStreamRef.current)
          outputAnalyserRef.current = audioContextRef.current.createAnalyser()
          outputAnalyserRef.current.fftSize = OUTPUT_ANALYSER_CONFIG.fftSize
          outputAnalyserRef.current.smoothingTimeConstant = OUTPUT_ANALYSER_CONFIG.smoothingTimeConstant
          outputAnalyserRef.current.minDecibels = OUTPUT_ANALYSER_CONFIG.minDecibels
          outputAnalyserRef.current.maxDecibels = OUTPUT_ANALYSER_CONFIG.maxDecibels
          outputSource.connect(outputAnalyserRef.current)
          console.log('‚úÖ Output analyser –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω')
        } catch (error) {
          console.error('‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è output analyser:', error)
        }
      }
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º HTML Audio Element
    if (audioElementRef.current) {
      const audioElement = audioElementRef.current
      
      if (audioElement.error) {
        console.log('üîß Health check: Audio element has error:', audioElement.error)
        needsRecovery = true
      }
      
      if (audioElement.networkState === HTMLMediaElement.NETWORK_NO_SOURCE) {
        console.log('üîß Health check: Audio element no source')
        needsRecovery = true
      }
    }
    
    if (needsRecovery) {
      console.log('üîß Health check –æ–±–Ω–∞—Ä—É–∂–∏–ª –ø—Ä–æ–±–ª–µ–º—ã, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥')
    }
  }

  // –°–æ–∑–¥–∞–µ–º Audio —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
  useEffect(() => {
    audioElementRef.current = new Audio()
    audioElementRef.current.autoplay = true
    
    // üîß –ù–û–í–û–ï: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º audio recovery system
    setupAudioElementRecovery(audioElementRef.current)
    
    // üîß –ù–û–í–û–ï: –∑–∞–ø—É—Å–∫–∞–µ–º health check –∫–∞–∂–¥—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã
    healthCheckIntervalRef.current = setInterval(performComprehensiveHealthCheck, 3000)
    
    return () => {
      if (audioElementRef.current) {
        audioElementRef.current.pause()
        audioElementRef.current = null
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      if (audioContextRef.current) {
        audioContextRef.current.close()
      }
      if (healthCheckIntervalRef.current) {
        clearInterval(healthCheckIntervalRef.current)
      }
    }
  }, [])

  // –§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ —Å —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ–º –∏ audio recovery
  const analyzeAudio = async () => {
    // üîß –ù–û–í–û–ï: –ø—Ä–æ–≤–µ—Ä—è–µ–º AudioContext –ø–µ—Ä–µ–¥ –∞–Ω–∞–ª–∏–∑–æ–º
    if (!(await ensureAudioContextActive())) {
      console.log('‚ö†Ô∏è AudioContext –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑')
      animationFrameRef.current = requestAnimationFrame(analyzeAudio)
      return
    }
    
    if (!inputAnalyserRef.current && !outputAnalyserRef.current) {
      animationFrameRef.current = requestAnimationFrame(analyzeAudio)
      return
    }

    const now = performance.now()
    
    // Throttling: –æ–±–Ω–æ–≤–ª—è–µ–º –Ω–µ —á–∞—â–µ 30 FPS –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
    if (now - lastVolumeUpdateRef.current < 33) {
      animationFrameRef.current = requestAnimationFrame(analyzeAudio)
      return
    }
    lastVolumeUpdateRef.current = now

    // –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∏ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
    const NOISE_GATE = 0.15  // –ü–æ—Ä–æ–≥ —à—É–º–∞ (15%)
    const SMOOTHING_FACTOR = 0.5  // –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
    const HISTORY_SIZE = 5  // –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è

    // –ê–Ω–∞–ª–∏–∑ –≤—Ö–æ–¥—è—â–µ–≥–æ –∑–≤—É–∫–∞ (–º–∏–∫—Ä–æ—Ñ–æ–Ω)
    if (inputAnalyserRef.current) {
      const bufferLength = inputAnalyserRef.current.frequencyBinCount
      const dataArray = new Uint8Array(bufferLength)
      inputAnalyserRef.current.getByteFrequencyData(dataArray)
      
      // –í—ã—á–∏—Å–ª—è–µ–º RMS (Root Mean Square) –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –≥—Ä–æ–º–∫–æ—Å—Ç–∏
      const rms = Math.sqrt(dataArray.reduce((sum, value) => sum + value * value, 0) / bufferLength)
      let volume = Math.min(rms / 128, 1)
      
      // –ü—Ä–∏–º–µ–Ω—è–µ–º noise gate - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ç–∏—Ö–∏–µ –∑–≤—É–∫–∏
      if (volume < NOISE_GATE) {
        volume = 0
      } else {
        // –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Ä–æ–≥–∞ —à—É–º–∞
        volume = (volume - NOISE_GATE) / (1 - NOISE_GATE)
      }
      
      // –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
      volumeHistoryRef.current.input.push(volume)
      if (volumeHistoryRef.current.input.length > HISTORY_SIZE) {
        volumeHistoryRef.current.input.shift()
      }
      
      // –í—ã—á–∏—Å–ª—è–µ–º —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
      const smoothedVolume = volumeHistoryRef.current.input.reduce((sum, v) => sum + v, 0) / volumeHistoryRef.current.input.length
      
      // –ü—Ä–∏–º–µ–Ω—è–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
      const finalVolume = smoothedVolume * SMOOTHING_FACTOR + (volumeHistoryRef.current.input[volumeHistoryRef.current.input.length - 1] || 0) * (1 - SMOOTHING_FACTOR)
      
      // –î–µ–±–∞–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö)
      if (Math.abs(finalVolume - (volumeHistoryRef.current.input[volumeHistoryRef.current.input.length - 2] || 0)) > 0.1) {
        console.log(`üé§ Input volume: ${(finalVolume * 100).toFixed(1)}%`);
      }
      
      onVolumeChange(finalVolume, 'input')
    }

    // –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥—è—â–µ–≥–æ –∑–≤—É–∫–∞ (–æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞)  
    if (outputAnalyserRef.current) {
      const bufferLength = outputAnalyserRef.current.frequencyBinCount
      const dataArray = new Uint8Array(bufferLength)
      outputAnalyserRef.current.getByteFrequencyData(dataArray)
      
      // –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –∑–≤—É–∫–∞
      const rms = Math.sqrt(dataArray.reduce((sum, value) => sum + value * value, 0) / bufferLength)
      let volume = Math.min(rms / 128, 1)
      
      // –û–¢–õ–ê–î–ö–ê: –≤—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º raw –¥–∞–Ω–Ω—ã–µ –¥–ª—è output
      if (rms > 5) {
        // –¢–∏—Ö–∏–π –∞–Ω–∞–ª–∏–∑ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ output
      }
      
      // –î–ª—è —Ä–µ—á–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
      const OUTPUT_NOISE_GATE = 0.08
      if (volume < OUTPUT_NOISE_GATE) {
        volume = 0
      } else {
        volume = (volume - OUTPUT_NOISE_GATE) / (1 - OUTPUT_NOISE_GATE)
      }
      
      volumeHistoryRef.current.output.push(volume)
      if (volumeHistoryRef.current.output.length > HISTORY_SIZE) {
        volumeHistoryRef.current.output.shift()
      }
      
      const smoothedVolume = volumeHistoryRef.current.output.reduce((sum, v) => sum + v, 0) / volumeHistoryRef.current.output.length
      const finalVolume = smoothedVolume * SMOOTHING_FACTOR + (volumeHistoryRef.current.output[volumeHistoryRef.current.output.length - 1] || 0) * (1 - SMOOTHING_FACTOR)
      
      // –î–µ–±–∞–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ - –í–°–ï–ì–î–ê –ø–æ–∫–∞–∑—ã–≤–∞–µ–º
      if (finalVolume > 0.05) {
        // –¢–∏—Ö–∏–π –∞–Ω–∞–ª–∏–∑ - –ª–æ–≥ —É–±—Ä–∞–Ω
      }
      
      // –£–±—Ä–∞–ª–∏ –ª–æ–≥–∏–∫—É –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π - –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ talking
      
      onVolumeChange(finalVolume, 'output')
    }

    animationFrameRef.current = requestAnimationFrame(analyzeAudio)
  }

  const connectToRealtimeAPI = async () => {
    if (isConnecting || isConnected) return;
    
    setIsConnecting(true);
    onVoiceStateChange('connecting');

    try {
      // –ü–æ–ª—É—á–∞–µ–º ephemeral token
      const response = await fetch('/api/session', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ systemPrompt }),
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const tokenData = await response.json();
      console.log('Token response:', tokenData);
      
      const token = tokenData.client_secret?.value || tokenData.client_secret || tokenData.token || tokenData.access_token;
      
      if (!token) {
        console.error('Token structure:', tokenData);
        throw new Error('No token found in response');
      }

      // –°–æ–∑–¥–∞–µ–º WebRTC —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
      const pc = new RTCPeerConnection(WEBRTC_CONFIG);
      
      // üîß –ù–û–í–û–ï: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ WebRTC —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
      pc.addEventListener('connectionstatechange', () => {
        console.log(`üîó WebRTC connection state: ${pc.connectionState}`)
        if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected') {
          console.log('‚ö†Ô∏è WebRTC —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–æ, –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ')
        }
      })
      
      pc.addEventListener('iceconnectionstatechange', () => {
        console.log(`üßä ICE connection state: ${pc.iceConnectionState}`)
        if (pc.iceConnectionState === 'failed' || pc.iceConnectionState === 'disconnected') {
          console.log('‚ö†Ô∏è ICE —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–æ')
        }
      })
      
      // –î–æ–±–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ —Ç—Ä–µ–∫ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
      const ms = await navigator.mediaDevices.getUserMedia({ 
        audio: MICROPHONE_CONFIG
      });
      pc.addTrack(ms.getTracks()[0], ms);

      // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤—Ö–æ–¥—è—â–µ–≥–æ –∞—É–¥–∏–æ (–º–∏–∫—Ä–æ—Ñ–æ–Ω)
      audioContextRef.current = new AudioContext();
      const source = audioContextRef.current.createMediaStreamSource(ms);
      inputAnalyserRef.current = audioContextRef.current.createAnalyser();
      
      // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—á–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
      inputAnalyserRef.current.fftSize = ANALYSER_CONFIG.fftSize;
      inputAnalyserRef.current.smoothingTimeConstant = ANALYSER_CONFIG.smoothingTimeConstant;
      inputAnalyserRef.current.minDecibels = ANALYSER_CONFIG.minDecibels;
      inputAnalyserRef.current.maxDecibels = ANALYSER_CONFIG.maxDecibels;
      
      source.connect(inputAnalyserRef.current);

      // –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥—è—â–∏—Ö –∞—É–¥–∏–æ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
      pc.ontrack = e => {
        remoteStreamRef.current = e.streams[0];
        console.log('üîä –ü–æ–ª—É—á–µ–Ω remote stream –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:', e.streams[0]);
        
        // üîß –ù–û–í–û–ï: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ MediaStream
        const stream = e.streams[0];
        const audioTracks = stream.getAudioTracks();
        
        if (audioTracks.length > 0) {
          const track = audioTracks[0];
          console.log(`üîä Audio track: ${track.label}, state: ${track.readyState}`);
          
          // –ú–æ–Ω–∏—Ç–æ—Ä–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ track'–∞
          track.addEventListener('ended', () => {
            console.log('‚ö†Ô∏è Audio track ended unexpectedly');
          });
          
          track.addEventListener('mute', () => {
            console.log('‚ö†Ô∏è Audio track muted');
          });
          
          track.addEventListener('unmute', () => {
            console.log('‚úÖ Audio track unmuted');
          });
        }
        
        // –ú–æ–Ω–∏—Ç–æ—Ä–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ç–æ–∫–∞
        stream.addEventListener('removetrack', (event) => {
          console.log('‚ö†Ô∏è Track removed from stream:', event.track);
        });
        
        stream.addEventListener('addtrack', (event) => {
          console.log('‚úÖ Track added to stream:', event.track);
        });
        
        if (audioElementRef.current) {
          audioElementRef.current.srcObject = stream;
          
          // üîß –£–õ–£–ß–®–ï–ù–û: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥—è—â–µ–≥–æ –∞—É–¥–∏–æ —Å error handling
          if (audioContextRef.current) {
            try {
              const outputSource = audioContextRef.current.createMediaStreamSource(stream);
              outputAnalyserRef.current = audioContextRef.current.createAnalyser();
              
              // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—á–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
              outputAnalyserRef.current.fftSize = OUTPUT_ANALYSER_CONFIG.fftSize;
              outputAnalyserRef.current.smoothingTimeConstant = OUTPUT_ANALYSER_CONFIG.smoothingTimeConstant;
              outputAnalyserRef.current.minDecibels = OUTPUT_ANALYSER_CONFIG.minDecibels;
              outputAnalyserRef.current.maxDecibels = OUTPUT_ANALYSER_CONFIG.maxDecibels;
              
              outputSource.connect(outputAnalyserRef.current);
              
              // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∞–π–º–µ—Ä –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö
              lastOutputDataRef.current = performance.now();
              
              console.log('üîä Output –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω —á–µ—Ä–µ–∑ MediaStream');
            } catch (error) {
              console.error('üîä –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ output –∞–Ω–∞–ª–∏–∑–∞:', error);
            }
          }
        }
      };

      // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º DataChannel listeners –ü–ï–†–ï–î offer/answer –ø—Ä–æ—Ü–µ—Å—Å–æ–º
      console.log('üîÑ –û–∂–∏–¥–∞–µ–º DataChannel –æ—Ç OpenAI...');
      pc.addEventListener('datachannel', (event) => {
        const dataChannel = event.channel;
        dataChannelRef.current = dataChannel;
        console.log('üì° ‚úÖ DataChannel –ø–æ–ª—É—á–µ–Ω –æ—Ç OpenAI:', dataChannel.label, dataChannel.readyState);

        dataChannel.addEventListener('message', (event) => {
          try {
            const realtimeEvent = JSON.parse(event.data);
            console.log('üì° üéØ VAD –°–û–ë–´–¢–ò–ï –æ—Ç OpenAI:', realtimeEvent.type, realtimeEvent);
            handleRealtimeEvent(realtimeEvent);
            // –¢–∏—Ö–∞—è —Ä–∞–±–æ—Ç–∞ - –æ—Ç–ª–∞–¥–æ—á–Ω—ã–µ –ª–æ–≥–∏ —É–±—Ä–∞–Ω—ã
          } catch (error) {
            console.error('‚ùå Error parsing realtime event:', error, event.data);
          }
        });

        dataChannel.addEventListener('open', () => {
          console.log('‚úÖ DataChannel –æ—Ç–∫—Ä—ã—Ç - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VAD');
          
          // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ—Å—Å–∏–∏ —Å VAD –∏ Tools
          const finalPrompt = buildFinalSystemPrompt(systemPrompt);
          const sessionUpdate = {
            type: 'session.update',
            session: {
              instructions: finalPrompt,
              voice: selectedVoice, // üéôÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å
              input_audio_format: 'pcm16',
              output_audio_format: 'pcm16',
              turn_detection: {
                type: 'server_vad',
                threshold: 0.5,
                prefix_padding_ms: 300,
                silence_duration_ms: 200
              },
              tools: AVAILABLE_TOOLS
            }
          };
          
          console.log('üì° –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VAD:', sessionUpdate);
          
          try {
            dataChannel.send(JSON.stringify(sessionUpdate));
            console.log('‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ VAD –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ');
          } catch (error) {
            console.error('‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ VAD:', error);
          }
        });

        dataChannel.addEventListener('error', (error) => {
          console.error('‚ùå DataChannel error:', error);
        });

        dataChannel.addEventListener('close', () => {
          console.log('üì° DataChannel –∑–∞–∫—Ä—ã—Ç');
        });
      });

      // –°–æ–∑–¥–∞–µ–º offer
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      // –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ OpenAI Realtime API (URL –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
      const sdpResponse = await fetch(REALTIME_API_URLs.websocket, {
        method: 'POST',
        body: offer.sdp,
        headers: {
          Authorization: `Bearer ${token}`,
          'Content-Type': 'application/sdp'
        },
      });

      if (!sdpResponse.ok) {
        throw new Error('Failed to connect to OpenAI Realtime API');
      }

      const answerSdp = await sdpResponse.text();
      await pc.setRemoteDescription({
        type: 'answer',
        sdp: answerSdp,
      });

      peerConnectionRef.current = pc;
      localStreamRef.current = ms;
      setIsConnected(true);
      onVoiceStateChange('talking');

      // –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
      analyzeAudio();

      // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å DataChannel —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã
      setTimeout(() => {
        const channelStatus = dataChannelRef.current?.readyState || 'not_created';
        console.log('üîç DataChannel —Å—Ç–∞—Ç—É—Å —á–µ—Ä–µ–∑ 3 —Å–µ–∫:', channelStatus);
        if (channelStatus !== 'open') {
          console.log('‚ö†Ô∏è DataChannel –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º FALLBACK –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≥—Ä–æ–º–∫–æ—Å—Ç–∏');
        }
      }, 3000);

      console.log('Successfully connected to OpenAI Realtime API');
    } catch (error) {
      console.error('Error connecting to Realtime API:', error);
      alert('–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.');
      setIsConnecting(false);
      onVoiceStateChange('disconnected');
    }
  };

  // –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π (tools)
  const handleToolCall = async (event: { name: string; call_id: string; arguments: string }) => {
    const { name, call_id, arguments: args } = event;
    console.log('üîß Tool call:', name, 'with args:', args);
    
    try {
      // –ù–∞—Ö–æ–¥–∏–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏
      const handler = TOOL_HANDLERS[name as keyof typeof TOOL_HANDLERS];
      if (!handler) {
        throw new Error(`Unknown tool: ${name}`);
      }
      
      // –í—ã–ø–æ–ª–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é
      const result = await handler(JSON.parse(args));
      console.log('üîß Tool result:', result);
      
      // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ –≤ OpenAI
      if (dataChannelRef.current && dataChannelRef.current.readyState === 'open') {
        const toolResponse = {
          type: 'conversation.item.create',
          item: {
            type: 'function_call_output',
            call_id: call_id,
            output: JSON.stringify(result)
          }
        };
        
        dataChannelRef.current.send(JSON.stringify(toolResponse));
        console.log('üîß Tool response sent:', toolResponse);
        
        // –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        const createResponse = {
          type: 'response.create'
        };
        dataChannelRef.current.send(JSON.stringify(createResponse));
        console.log('üîß Requested new response from model');
      }
    } catch (error) {
      console.error('üîß Tool execution error:', error);
      
      // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –æ–±—Ä–∞—Ç–Ω–æ –≤ OpenAI
      if (dataChannelRef.current && dataChannelRef.current.readyState === 'open') {
        const errorResponse = {
          type: 'conversation.item.create',
          item: {
            type: 'function_call_output',
            call_id: call_id,
            output: JSON.stringify({
              success: false,
              error: error instanceof Error ? error.message : 'Unknown error'
            })
          }
        };
        
        dataChannelRef.current.send(JSON.stringify(errorResponse));
        console.log('üîß Tool error response sent:', errorResponse);
      }
    }
  };

  const handleRealtimeEvent = (event: { type: string; [key: string]: unknown }) => {
    console.log('üé™ handleRealtimeEvent –í–´–ó–í–ê–ù:', event.type);
    switch (event.type) {
      case 'session.created':
        console.log('üîÑ –°–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞:', event.session);
        break;
        
      case 'conversation.item.created':
        console.log('üí¨ –≠–ª–µ–º–µ–Ω—Ç –±–µ—Å–µ–¥—ã —Å–æ–∑–¥–∞–Ω:', event.item);
        break;
        
      case 'response.created':
        console.log('üéØ –û—Ç–≤–µ—Ç —Å–æ–∑–¥–∞–Ω:', event.response);
        break;
        
      case 'response.output_item.added':
        console.log('üì§ –í—ã—Ö–æ–¥–Ω–æ–π —ç–ª–µ–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω:', event.item);
        break;
        
      case 'response.audio.delta':
        console.log('üéµ VAD: –ò–ò –Ω–∞—á–∞–ª –≥–æ–≤–æ—Ä–∏—Ç—å - –∞—É–¥–∏–æ –¥–µ–ª—å—Ç–∞');
        break;
        
      case 'response.audio.done':
        console.log('üéµ VAD: –ò–ò –∑–∞–∫–æ–Ω—á–∏–ª –≥–æ–≤–æ—Ä–∏—Ç—å - –∞—É–¥–∏–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ');
        break;
        
      case 'input_audio_buffer.speech_started':
        console.log('üé§ VAD: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞—á–∞–ª –≥–æ–≤–æ—Ä–∏—Ç—å');
        break;
        
      case 'input_audio_buffer.speech_stopped':
        console.log('üé§ VAD: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–∫–æ–Ω—á–∏–ª –≥–æ–≤–æ—Ä–∏—Ç—å');
        break;
        
      case 'response.done':
        console.log('üèÅ VAD: –û—Ç–≤–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω:', event.response);
        break;
        
      // === –û–ë–†–ê–ë–û–¢–ö–ê FUNCTION CALLS ===
      case 'response.function_call_arguments.delta':
        console.log('üîß Tool call arguments delta:', event);
        break;
        
      case 'response.function_call_arguments.done':
        console.log('üîß Tool call arguments ready:', event);
        handleToolCall(event as unknown as { name: string; call_id: string; arguments: string });
        break;
        
      default:
        console.log('üì® –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ:', event.type);
        break;
    }
  };

  const disconnect = () => {
    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    if (peerConnectionRef.current) {
      peerConnectionRef.current.close();
      peerConnectionRef.current = null;
    }
    if (localStreamRef.current) {
      localStreamRef.current.getTracks().forEach((track: MediaStreamTrack) => track.stop());
      localStreamRef.current = null;
    }
    if (dataChannelRef.current) {
      dataChannelRef.current = null;
    }
    
    inputAnalyserRef.current = null;
    outputAnalyserRef.current = null;
    
    // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –≥—Ä–æ–º–∫–æ—Å—Ç–∏
    volumeHistoryRef.current = { input: [], output: [] };
    lastVolumeUpdateRef.current = 0;
    
    // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –í–°–ï —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    setIsConnected(false);
    setIsConnecting(false);
    onVoiceStateChange('disconnected');
    onVolumeChange(0, 'input');
    onVolumeChange(0, 'output');
  };

  const toggleMute = () => {
    if (audioElementRef.current) {
      audioElementRef.current.muted = !isMuted;
      setIsMuted(!isMuted);
    }
  };

  return (
    <div className="space-y-10">
      {/* Welcome –∑–∞–≥–æ–ª–æ–≤–æ–∫ */}
      <div className="text-center max-w-md mx-auto animate-fade-in-up">
        <h2 className="heading-secondary mb-4">–ò–ò –ì–æ–ª–æ—Å–æ–≤–æ–π –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç</h2>
        <p className="text-body mb-8">
          –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É&nbsp;&ldquo;–ù–∞—á–∞—Ç—å&nbsp;—Ä–∞–∑–≥–æ–≤–æ—Ä&rdquo; –∏&nbsp;–æ–±—â–∞–π—Ç–µ—Å—å<br />
          —Å&nbsp;–ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º –≥–æ–ª–æ—Å–æ–º.
        </p>
      </div>

      {/* VoiceOrb - —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç */}
      <div className="flex justify-center mb-12">
        <VoiceOrb 
          state={voiceState}
          inputVolume={inputVolume}
          outputVolume={outputVolume}
        />
      </div>

      {/* –ë–µ–ª–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞ —Å –Ω–∏–∂–Ω–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ */}
      <div className="max-w-2xl mx-auto px-4">
        <div className="card p-6 md:p-8 space-y-6 animate-fade-in-up inline-block w-full" style={{animationDelay: '0.2s'}}>
          {/* –°—Ç–∞—Ç—É—Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è */}
          <div className="text-center">
            <div className={`status-indicator ${
              isConnected 
                ? 'status-connected' 
                : isConnecting 
                ? 'status-connecting animate-pulse-soft'
                : 'status-disconnected'
            }`}>
              <div className={`w-2 h-2 rounded-full mr-3 ${
                isConnected 
                  ? 'bg-green-500' 
                  : isConnecting 
                  ? 'bg-yellow-500'
                  : 'bg-gray-500'
              }`} />
              {isConnected ? '–ü–æ–¥–∫–ª—é—á–µ–Ω' : isConnecting ? '–ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è...' : '–û—Ç–∫–ª—é—á–µ–Ω'}
            </div>
          </div>

          {/* –ì–æ–ª–æ—Å–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è */}
      <div className="flex items-center justify-center space-x-6">
        {!isConnected ? (
          <button
            onClick={connectToRealtimeAPI}
            disabled={isConnecting}
            className={`btn-primary ${isConnecting ? 'animate-pulse-soft' : ''}`}
          >
            <div className="flex items-center space-x-3">
              <svg className="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
              </svg>
              <span>{isConnecting ? '–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ...' : '–ù–∞—á–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä'}</span>
            </div>
          </button>
        ) : (
          <>
            <button
              onClick={disconnect}
              className="bg-gradient-to-r from-red-500 to-pink-600 hover:from-red-600 hover:to-pink-700 text-white font-semibold py-4 px-8 rounded-2xl transform transition-all duration-200 ease-in-out hover:scale-105 hover:shadow-lg focus:outline-none focus:ring-4 focus:ring-red-300 btn-disconnect"
            >
              <div className="flex items-center space-x-3">
                <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
                </svg>
                <span>–û—Ç–∫–ª—é—á–∏—Ç—å—Å—è</span>
              </div>
            </button>
            
            <button
              onClick={toggleMute}
              className={`glass p-4 rounded-2xl hover-lift transition-all duration-200 ${
                isMuted 
                  ? 'text-red-600 bg-red-50/50' 
                  : 'text-gray-600 hover:text-gray-800'
              }`}
              title={isMuted ? '–í–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫' : '–í—ã–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫'}
            >
              {isMuted ? (
                // –ò–∫–æ–Ω–∫–∞ "–∑–≤—É–∫ –≤—ã–∫–ª—é—á–µ–Ω" - –¥–∏–Ω–∞–º–∏–∫ –ø–µ—Ä–µ—á–µ—Ä–∫–Ω—É—Ç—ã–π
                <svg className="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M5.586 15H4a1 1 0 01-1-1v-4a1 1 0 011-1h1.586l4.707-4.707C10.923 3.663 12 4.109 12 5v14c0 .891-1.077 1.337-1.707.707L5.586 15z" />
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M18 3L1 20" />
                </svg>
              ) : (
                // –ò–∫–æ–Ω–∫–∞ "–∑–≤—É–∫ –≤–∫–ª—é—á–µ–Ω" - –¥–∏–Ω–∞–º–∏–∫ —Å –≤–æ–ª–Ω–∞–º–∏ –∑–≤—É–∫–∞
                <svg className="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M5.586 15H4a1 1 0 01-1-1v-4a1 1 0 011-1h1.586l4.707-4.707C10.923 3.663 12 4.109 12 5v14c0 .891-1.077 1.337-1.707.707L5.586 15z" />
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M15 9.5c.5.5 1 1.5 1 2.5s-.5 2-1 2.5" />
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M18 7.5c1 1 2 2.5 2 4.5s-1 3.5-2 4.5" />
                </svg>
              )}
            </button>
          </>
        )}
      </div>

          {/* –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ */}
          {isConnected && (
            <div className="text-center">
              <div className="glass p-4 rounded-2xl max-w-xs mx-auto">
                <div className="flex items-center justify-center space-x-3">
                  <div className="w-3 h-3 rounded-full bg-green-500 animate-pulse-soft" />
                  <span className="text-sm font-medium text-green-800">
                    –†–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ–º
                  </span>
                </div>
              </div>
            </div>
          )}
        </div>
      </div>
    </div>
  )
}